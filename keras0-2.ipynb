{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 3, 32, 32)\n",
      "50000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "#Train a simple deep CNN on the CIFAR10 small images dataset.\n",
    "\n",
    "It gets to 75% validation accuracy in 25 epochs, and 79% after 50 epochs.\n",
    "(it's still underfitting at that point, though).\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "import os\n",
    "\n",
    "batch_size = 1000\n",
    "num_classes = 10\n",
    "epochs = 10\n",
    "data_augmentation = True\n",
    "num_predictions = 20\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "model_name = 'keras_cifar10_trained_model.h5'\n",
    "\n",
    "# The data, split between train and test sets:\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = np_utils.to_categorical(y_train, num_classes)\n",
    "y_test = np_utils.to_categorical(y_test, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 32, 32)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape[1:])\n",
    "print(type(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb2gray(rgb):\n",
    "    r, g, b = rgb[:,0,:,:], rgb[:,1,:,:], rgb[:,2,:,:]\n",
    "    gray = 0.2989 * r + 0.5870 * g + 0.1140 * b\n",
    "    gray = np.expand_dims(gray, axis=1)\n",
    "    print(gray.shape)\n",
    "    return gray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 1, 32, 32)\n",
      "(10000, 1, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "x_train=rgb2gray(x_train)\n",
    "x_test=rgb2gray(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 1, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/miniconda3/lib/python3.5/site-packages/ipykernel_launcher.py:2: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(2, (5, 5), padding=\"valid\", input_shape=(1, 32, 32...)`\n",
      "  \n",
      "/usr/local/miniconda3/lib/python3.5/site-packages/ipykernel_launcher.py:5: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (5, 5), padding=\"valid\")`\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Convolution2D(2, 5, 5, border_mode='valid', input_shape=(1, 32, 32)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Activation('relu'))   \n",
    "model.add(Convolution2D(16, 5, 5, border_mode='valid'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(120))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(84))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 32, 32)\n",
      "[[[  61.2111   44.9847   48.023  ...,  137.0222  130.4358  128.7672]\n",
      "  [  18.8024    0.       10.0762 ...,   94.6907   89.9901   94.0328]\n",
      "  [  23.9545    8.8914   31.4071 ...,   90.2782   90.876    80.2191]\n",
      "  ..., \n",
      "  [ 172.9052  153.7659  156.6532 ...,  133.875    35.7334   38.0797]\n",
      "  [ 146.339   128.6987  143.5434 ...,  152.5896   69.2633   59.7957]\n",
      "  [ 150.6573  136.6542  146.7751 ...,  188.5304  123.9759   98.9767]]]\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0].shape)\n",
    "print(x_train[0,:,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "def neg_log_likelihood_loss(y_true, y_pred):\n",
    "    ##sep = y_pred.shape[1] // 2\n",
    "    mu, logvar = y_pred[:, :10], y_pred[:, 10:]\n",
    "    return K.sum(0.5*(logvar+np.log(2*np.pi)+K.square((y_true-mu)/K.exp(0.5*logvar))), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate RMSprop optimizer\n",
    "sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['categorical_crossentropy', 'accuracy'])\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/miniconda3/lib/python3.5/site-packages/keras/models.py:848: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "50000/50000 [==============================] - 0s - loss: 2.2437 - categorical_crossentropy: 2.2437 - acc: 0.1612     \n",
      "Epoch 2/10\n",
      "50000/50000 [==============================] - 0s - loss: 1.9468 - categorical_crossentropy: 1.9468 - acc: 0.3074     \n",
      "Epoch 3/10\n",
      "50000/50000 [==============================] - 0s - loss: 1.8006 - categorical_crossentropy: 1.8006 - acc: 0.3629     \n",
      "Epoch 4/10\n",
      "50000/50000 [==============================] - 0s - loss: 1.7161 - categorical_crossentropy: 1.7161 - acc: 0.3938     \n",
      "Epoch 5/10\n",
      "50000/50000 [==============================] - 0s - loss: 1.6368 - categorical_crossentropy: 1.6368 - acc: 0.4198     \n",
      "Epoch 6/10\n",
      "50000/50000 [==============================] - 0s - loss: 1.5957 - categorical_crossentropy: 1.5957 - acc: 0.4397     \n",
      "Epoch 7/10\n",
      "50000/50000 [==============================] - 0s - loss: 1.5307 - categorical_crossentropy: 1.5307 - acc: 0.4613     \n",
      "Epoch 8/10\n",
      "50000/50000 [==============================] - 0s - loss: 1.5008 - categorical_crossentropy: 1.5008 - acc: 0.4751     \n",
      "Epoch 9/10\n",
      "50000/50000 [==============================] - 0s - loss: 1.4580 - categorical_crossentropy: 1.4580 - acc: 0.4859     \n",
      "Epoch 10/10\n",
      "50000/50000 [==============================] - 0s - loss: 1.4134 - categorical_crossentropy: 1.4134 - acc: 0.5058     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4c34245a90>"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.fit(x_train, y_train,\n",
    "#          batch_size=batch_size,\n",
    "#          nb_epoch=epochs,\n",
    "#          #validation_data=(x_test, y_test),\n",
    "#          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using real-time data augmentation.\n",
      "Epoch 1/10\n",
      "50/50 [==============================] - 4s - loss: 1.5902 - categorical_crossentropy: 1.5902 - acc: 0.4423 - val_loss: 1.8923 - val_categorical_crossentropy: 1.8923 - val_acc: 0.3605\n",
      "Epoch 2/10\n",
      "50/50 [==============================] - 3s - loss: 1.5806 - categorical_crossentropy: 1.5806 - acc: 0.4462 - val_loss: 1.5020 - val_categorical_crossentropy: 1.5020 - val_acc: 0.4704\n",
      "Epoch 3/10\n",
      "50/50 [==============================] - 4s - loss: 1.5372 - categorical_crossentropy: 1.5372 - acc: 0.4608 - val_loss: 1.4260 - val_categorical_crossentropy: 1.4260 - val_acc: 0.4978\n",
      "Epoch 4/10\n",
      "50/50 [==============================] - 4s - loss: 1.5181 - categorical_crossentropy: 1.5181 - acc: 0.4644 - val_loss: 1.4149 - val_categorical_crossentropy: 1.4149 - val_acc: 0.5020\n",
      "Epoch 5/10\n",
      "50/50 [==============================] - 4s - loss: 1.5179 - categorical_crossentropy: 1.5179 - acc: 0.4671 - val_loss: 1.3963 - val_categorical_crossentropy: 1.3963 - val_acc: 0.5100\n",
      "Epoch 6/10\n",
      "50/50 [==============================] - 4s - loss: 1.4863 - categorical_crossentropy: 1.4863 - acc: 0.4784 - val_loss: 1.4152 - val_categorical_crossentropy: 1.4152 - val_acc: 0.5067\n",
      "Epoch 7/10\n",
      "50/50 [==============================] - 4s - loss: 1.5089 - categorical_crossentropy: 1.5089 - acc: 0.4720 - val_loss: 1.4022 - val_categorical_crossentropy: 1.4022 - val_acc: 0.5071\n",
      "Epoch 8/10\n",
      "50/50 [==============================] - 4s - loss: 1.4494 - categorical_crossentropy: 1.4494 - acc: 0.4908 - val_loss: 1.4172 - val_categorical_crossentropy: 1.4172 - val_acc: 0.5072\n",
      "Epoch 9/10\n",
      "50/50 [==============================] - 4s - loss: 1.4587 - categorical_crossentropy: 1.4587 - acc: 0.4893 - val_loss: 1.3699 - val_categorical_crossentropy: 1.3699 - val_acc: 0.5252\n",
      "Epoch 10/10\n",
      "50/50 [==============================] - 4s - loss: 1.8993 - categorical_crossentropy: 1.8993 - acc: 0.3322 - val_loss: 1.7109 - val_categorical_crossentropy: 1.7109 - val_acc: 0.3909\n"
     ]
    }
   ],
   "source": [
    "if not data_augmentation:\n",
    "    print('Not using data augmentation.')\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True)\n",
    "else:\n",
    "    print('Using real-time data augmentation.')\n",
    "    # This will do preprocessing and realtime data augmentation:\n",
    "    datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "\n",
    "    # Compute quantities required for feature-wise normalization\n",
    "    # (std, mean, and principal components if ZCA whitening is applied).\n",
    "    datagen.fit(x_train)\n",
    "\n",
    "    # Fit the model on the batches generated by datagen.flow().\n",
    "    model.fit_generator(datagen.flow(x_train, y_train,\n",
    "                                     batch_size=batch_size),\n",
    "                        steps_per_epoch=x_train.shape[0] // batch_size,\n",
    "                        nb_epoch=epochs,\n",
    "                        validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy = 0.38, Model loss = 1.76 \n"
     ]
    }
   ],
   "source": [
    "# Evaluate model with test data set and share sample prediction results\n",
    "evaluation = model.evaluate_generator(datagen.flow(x_test, y_test,\n",
    "                                      batch_size=batch_size),\n",
    "                                      steps=x_test.shape[0] // batch_size)\n",
    "print('Model Accuracy = %.2f, Model loss = %.2f ' % (evaluation[2], evaluation[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved trained model at /root/shared/saved_models/keras_cifar10_trained_model.h5 \n",
      " 2000/10000 [=====>........................] - ETA: 0s[1.5061332702636718, 1.5061332702636718, 0.47669999599456786]\n"
     ]
    }
   ],
   "source": [
    "# Save model and weights\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "##model.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path)\n",
    "\n",
    "# Score trained model.\n",
    "#scores = model.evaluate(x_test, y_test,show_accuracy=True, verbose=1, batch_size=1000)\n",
    "scores = model.evaluate(x_test, y_test, verbose=1, batch_size=2000)\n",
    "print(scores)\n",
    "#print('Test loss:', scores[0])\n",
    "#print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n"
     ]
    }
   ],
   "source": [
    "print(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
