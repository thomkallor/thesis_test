{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "#Train a simple deep CNN on the CIFAR10 small images dataset.\n",
    "\n",
    "It gets to 75% validation accuracy in 25 epochs, and 79% after 50 epochs.\n",
    "(it's still underfitting at that point, though).\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Convolution2D, MaxPooling2D\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras import utils\n",
    "import os\n",
    "\n",
    "batch_size = 1000\n",
    "num_classes = 10\n",
    "epochs = 10\n",
    "data_augmentation = True\n",
    "num_predictions = 20\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "model_name = 'keras_cifar10_trained_model.h5'\n",
    "\n",
    "# The data, split between train and test sets:\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = utils.to_categorical(y_train, num_classes)\n",
    "y_test = utils.to_categorical(y_test, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 32, 3)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape[1:])\n",
    "print(type(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb2gray(rgb):\n",
    "    r, g, b = rgb[:,:,:,0], rgb[:,:,:,1], rgb[:,:,:,2]\n",
    "    gray = 0.2989 * r + 0.5870 * g + 0.1140 * b\n",
    "    gray = np.expand_dims(gray, axis=3)\n",
    "    print(gray.shape)\n",
    "    return gray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 1)\n",
      "(10000, 32, 32, 1)\n"
     ]
    }
   ],
   "source": [
    "x_train=rgb2gray(x_train)\n",
    "x_test=rgb2gray(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 32, 1)\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 32, 32, 1)\n"
     ]
    }
   ],
   "source": [
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Convolution2D(6, (5, 5), padding='valid', data_format=\"channels_last\", input_shape=(32, 32, 1)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Activation('relu'))   \n",
    "model.add(Convolution2D(16, (5, 5), padding='valid', data_format=\"channels_last\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(120))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(84))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 32, 1)\n",
      "[[[ 61.2111]\n",
      "  [ 44.9847]\n",
      "  [ 48.023 ]\n",
      "  ...\n",
      "  [137.0222]\n",
      "  [130.4358]\n",
      "  [128.7672]]\n",
      "\n",
      " [[ 18.8024]\n",
      "  [  0.    ]\n",
      "  [ 10.0762]\n",
      "  ...\n",
      "  [ 94.6907]\n",
      "  [ 89.9901]\n",
      "  [ 94.0328]]\n",
      "\n",
      " [[ 23.9545]\n",
      "  [  8.8914]\n",
      "  [ 31.4071]\n",
      "  ...\n",
      "  [ 90.2782]\n",
      "  [ 90.876 ]\n",
      "  [ 80.2191]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[172.9052]\n",
      "  [153.7659]\n",
      "  [156.6532]\n",
      "  ...\n",
      "  [133.875 ]\n",
      "  [ 35.7334]\n",
      "  [ 38.0797]]\n",
      "\n",
      " [[146.339 ]\n",
      "  [128.6987]\n",
      "  [143.5434]\n",
      "  ...\n",
      "  [152.5896]\n",
      "  [ 69.2633]\n",
      "  [ 59.7957]]\n",
      "\n",
      " [[150.6573]\n",
      "  [136.6542]\n",
      "  [146.7751]\n",
      "  ...\n",
      "  [188.5304]\n",
      "  [123.9759]\n",
      "  [ 98.9767]]]\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0].shape)\n",
    "print(x_train[0,:,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "def neg_log_likelihood_loss(y_true, y_pred):\n",
    "    ##sep = y_pred.shape[1] // 2\n",
    "    mu, logvar = y_pred[:, :10], y_pred[:, 10:]\n",
    "    return K.sum(0.5*(logvar+np.log(2*np.pi)+K.square((y_true-mu)/K.exp(0.5*logvar))), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate RMSprop optimizer\n",
    "sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['categorical_crossentropy', 'accuracy'])\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(x_train, y_train,\n",
    "#          batch_size=batch_size,\n",
    "#          nb_epoch=epochs,\n",
    "#          #validation_data=(x_test, y_test),\n",
    "#          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using real-time data augmentation.\n",
      "Epoch 1/10\n",
      "50/50 [==============================] - 7s 148ms/step - loss: 2.2344 - categorical_crossentropy: 2.2344 - accuracy: 0.1636 - val_loss: 2.0740 - val_categorical_crossentropy: 2.0740 - val_accuracy: 0.2562\n",
      "Epoch 2/10\n",
      "50/50 [==============================] - 7s 146ms/step - loss: 2.0700 - categorical_crossentropy: 2.0700 - accuracy: 0.2510 - val_loss: 1.8790 - val_categorical_crossentropy: 1.8790 - val_accuracy: 0.3225\n",
      "Epoch 3/10\n",
      "50/50 [==============================] - 7s 147ms/step - loss: 1.9323 - categorical_crossentropy: 1.9323 - accuracy: 0.3065 - val_loss: 1.8027 - val_categorical_crossentropy: 1.8027 - val_accuracy: 0.3555\n",
      "Epoch 4/10\n",
      "50/50 [==============================] - 7s 148ms/step - loss: 1.9586 - categorical_crossentropy: 1.9586 - accuracy: 0.2990 - val_loss: 1.8626 - val_categorical_crossentropy: 1.8626 - val_accuracy: 0.3335\n",
      "Epoch 5/10\n",
      "50/50 [==============================] - 7s 148ms/step - loss: 1.8742 - categorical_crossentropy: 1.8742 - accuracy: 0.3324 - val_loss: 1.7738 - val_categorical_crossentropy: 1.7738 - val_accuracy: 0.3647\n",
      "Epoch 6/10\n",
      "50/50 [==============================] - 7s 148ms/step - loss: 1.8337 - categorical_crossentropy: 1.8337 - accuracy: 0.3489 - val_loss: 1.7378 - val_categorical_crossentropy: 1.7378 - val_accuracy: 0.3839\n",
      "Epoch 7/10\n",
      "50/50 [==============================] - 7s 149ms/step - loss: 1.7814 - categorical_crossentropy: 1.7814 - accuracy: 0.3681 - val_loss: 1.7329 - val_categorical_crossentropy: 1.7329 - val_accuracy: 0.3840\n",
      "Epoch 8/10\n",
      "50/50 [==============================] - 7s 148ms/step - loss: 1.7501 - categorical_crossentropy: 1.7501 - accuracy: 0.3776 - val_loss: 1.6723 - val_categorical_crossentropy: 1.6723 - val_accuracy: 0.4063\n",
      "Epoch 9/10\n",
      "50/50 [==============================] - 7s 149ms/step - loss: 1.7240 - categorical_crossentropy: 1.7240 - accuracy: 0.3863 - val_loss: 1.6260 - val_categorical_crossentropy: 1.6260 - val_accuracy: 0.4174\n",
      "Epoch 10/10\n",
      "50/50 [==============================] - 7s 147ms/step - loss: 1.7978 - categorical_crossentropy: 1.7978 - accuracy: 0.3654 - val_loss: 1.7405 - val_categorical_crossentropy: 1.7405 - val_accuracy: 0.3806\n"
     ]
    }
   ],
   "source": [
    "if not data_augmentation:\n",
    "    print('Not using data augmentation.')\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True)\n",
    "else:\n",
    "    print('Using real-time data augmentation.')\n",
    "    # This will do preprocessing and realtime data augmentation:\n",
    "    datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "\n",
    "    # Compute quantities required for feature-wise normalization\n",
    "    # (std, mean, and principal components if ZCA whitening is applied).\n",
    "    datagen.fit(x_train)\n",
    "\n",
    "    # Fit the model on the batches generated by datagen.flow().\n",
    "    model.fit_generator(datagen.flow(x_train, y_train,\n",
    "                                     batch_size=batch_size),\n",
    "                        steps_per_epoch=x_train.shape[0] // batch_size,\n",
    "                        epochs=epochs,\n",
    "                        validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-94-28d7441799de>:4: Model.evaluate_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.evaluate, which supports generators.\n",
      "Model Accuracy = 0.36, Model loss = 1.80 \n"
     ]
    }
   ],
   "source": [
    "# Evaluate model with test data set and share sample prediction results\n",
    "evaluation = model.evaluate_generator(datagen.flow(x_test, y_test,\n",
    "                                      batch_size=batch_size),\n",
    "                                      steps=x_test.shape[0] // batch_size)\n",
    "print('Model Accuracy = %.2f, Model loss = %.2f ' % (evaluation[2], evaluation[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved trained model at /root/shared/saved_models/keras_cifar10_trained_model.h5 \n",
      "5/5 [==============================] - 0s 4ms/step - loss: 1.7405 - categorical_crossentropy: 1.7405 - accuracy: 0.3806\n",
      "[1.7404773235321045, 1.7404773235321045, 0.3806000053882599]\n"
     ]
    }
   ],
   "source": [
    "# Save model and weights\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "##model.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path)\n",
    "\n",
    "# Score trained model.\n",
    "#scores = model.evaluate(x_test, y_test,show_accuracy=True, verbose=1, batch_size=1000)\n",
    "scores = model.evaluate(x_test, y_test, verbose=1, batch_size=2000)\n",
    "print(scores)\n",
    "#print('Test loss:', scores[0])\n",
    "#print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n"
     ]
    }
   ],
   "source": [
    "print(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
